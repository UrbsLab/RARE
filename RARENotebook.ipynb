{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import choice\n",
    "from random import seed\n",
    "from random import randrange\n",
    "import collections\n",
    "import statistics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from skrebate import MultiSURF\n",
    "import math\n",
    "import numpy as numpy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Initialize Population of Candidate Bins\n",
    "\n",
    "#Random initialization of candidate bins, which are groupings of multiple features\n",
    "#The value of each bin/feature is the sum of values for each amino acid in the bin (0 for match, 1 for different)\n",
    "#Adding a function that can be an option to automatically separate rare and common variables\n",
    "\n",
    "#Assigning a random seed to allow for reproducible runs\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to delete variables with MAF = 0\n",
    "def Remove_Empty_Variables (original_feature_matrix, label_name):\n",
    "    #Removing the label column to create a list of features\n",
    "    feature_df = original_feature_matrix.drop(columns = [label_name])\n",
    "    \n",
    "    #Creating a list of features \n",
    "    feature_list = []\n",
    "    for column in feature_df:\n",
    "        feature_list.append(str(column))\n",
    "\n",
    "    feature_matrix_no_empty_variables = pd.DataFrame()\n",
    "    \n",
    "    #Creating a list of features with MAF = 0 to delete\n",
    "    MAF_0_features = []\n",
    "    \n",
    "    for i in range (0, len(feature_list)):\n",
    "        #If the MAF of the feature is less than the cutoff, it will be removed\n",
    "        if feature_df[feature_list[i]].sum()/(2*len(feature_df.index)) == 0:\n",
    "            MAF_0_features.append(feature_list[i])\n",
    "    \n",
    "    #Removing the features\n",
    "    for j in range (0, len(MAF_0_features)):\n",
    "        feature_list.remove(MAF_0_features[j])\n",
    "    \n",
    "    #Updating the feature matrix accordingly\n",
    "    for k in range (0, len(feature_list)):\n",
    "        feature_matrix_no_empty_variables[feature_list[k]] = feature_df[feature_list[k]]\n",
    "    \n",
    "    #Adding the class label to the feature matrix\n",
    "    feature_matrix_no_empty_variables['Class'] = original_feature_matrix[label_name]\n",
    "    \n",
    "    #Saving the feature list of nonempty features\n",
    "    nonempty_feature_list = feature_list\n",
    "    \n",
    "    return feature_matrix_no_empty_variables, MAF_0_features, nonempty_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to group features randomly, each feature can be in a number of groups up to a set max\n",
    "\n",
    "def Random_Feature_Grouping(feature_matrix, label_name, number_of_groups, min_features_per_group, \n",
    "                            max_number_of_groups_with_feature):\n",
    "    \n",
    "    #Removing the label column to create a list of features\n",
    "    feature_df = feature_matrix.drop(columns = [label_name])\n",
    "    \n",
    "    #Creating a list of features \n",
    "    feature_list = []\n",
    "    for column in feature_df:\n",
    "        feature_list.append(str(column))\n",
    "    \n",
    "    #Adding a random number of repeats of the features so that features can be in more than one group\n",
    "    for w in range (0, len(feature_list)):\n",
    "        repeats = randrange(max_number_of_groups_with_feature)\n",
    "        for i in range (0, repeats):\n",
    "            feature_list.append(feature_list[w])\n",
    "    \n",
    "    #Shuffling the feature list to enable random groups\n",
    "    random.shuffle(feature_list)\n",
    "    \n",
    "    #Creating a dictionary of the groups\n",
    "    feature_groups = {}\n",
    "    \n",
    "    #Assigns the minimum number of features to all the groups\n",
    "    for x in range (0, min_features_per_group*number_of_groups, min_features_per_group):\n",
    "        feature_groups[x/min_features_per_group] = feature_list[x:x+min_features_per_group]\n",
    "    \n",
    "    #Randomly distributes the remaining features across the set number of groups\n",
    "    for y in range (min_features_per_group*number_of_groups, len(feature_list)):\n",
    "        feature_groups[random.choice(list(feature_groups.keys()))].append(feature_list[y])\n",
    "    \n",
    "    #Removing duplicates of features in the same bin\n",
    "    for z in range (0, len(feature_groups)):\n",
    "        unique = []\n",
    "        for a in range (0, len(feature_groups[z])):\n",
    "            if feature_groups[z][a] not in unique:\n",
    "                unique.append(feature_groups[z][a])\n",
    "        feature_groups[z] = unique\n",
    "    \n",
    "    #Creating a dictionary with bin labels\n",
    "    binned_feature_groups = {}\n",
    "    for index in range (0, len(feature_groups)):\n",
    "        binned_feature_groups[\"Bin \" + str(index + 1)] = feature_groups[index]\n",
    "    \n",
    "    return feature_list, binned_feature_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to create a feature matrix where each feature is a bin of features from the original feature matrix\n",
    "\n",
    "def Grouped_Feature_Matrix(feature_matrix, label_name, binned_feature_groups):\n",
    "    \n",
    "    #Creating an empty data frame for the feature matrix with bins\n",
    "    bins_df = pd.DataFrame()\n",
    "    \n",
    "    #Creating a list of 0s, where the number of 0s is the number of instances in the original feature matrix\n",
    "    zero_list = []\n",
    "    for a in range (0, len(feature_matrix.index)):\n",
    "        zero_list.append(0)\n",
    "        \n",
    "    #Creating a dummy data frame\n",
    "    dummy_df = pd.DataFrame()\n",
    "    dummy_df['Zeros'] = zero_list\n",
    "    #The list and dummy data frame will be used for adding later\n",
    "    \n",
    "   #For each feature group/bin, the values of the amino acid in the bin will be summed to create a value for the bin \n",
    "    count = 0\n",
    "    for key in binned_feature_groups:\n",
    "        sum_column = dummy_df['Zeros']\n",
    "        for j in range (0, len(binned_feature_groups[key])):\n",
    "            sum_column = sum_column + feature_matrix[binned_feature_groups[key][j]]\n",
    "        count = count + 1\n",
    "        bins_df[key] = sum_column\n",
    "    \n",
    "    #Adding the class label to the data frame\n",
    "    bins_df['Class'] = feature_matrix[label_name]\n",
    "    return bins_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to separate rare and common variables based on a rare variant minor allele frequency (MAF) cutoff\n",
    "def Rare_and_Common_Variable_Separation (original_feature_matrix, label_name, rare_variant_MAF_cutoff):\n",
    "    \n",
    "    #Removing the label column to create a list of features\n",
    "    feature_df = original_feature_matrix.drop(columns = [label_name])\n",
    "    \n",
    "    #Creating a list of features \n",
    "    feature_list = []\n",
    "    for column in feature_df:\n",
    "        feature_list.append(str(column))\n",
    "    \n",
    "    #Creating lists of rare and common features\n",
    "    rare_feature_list = []\n",
    "    common_feature_list = []\n",
    "    MAF_0_features = []\n",
    "    \n",
    "    #Creating dictionaries of rare and common features, as the MAF of the features will be useful later\n",
    "    rare_feature_MAF_dict = {}\n",
    "    common_feature_MAF_dict = {}\n",
    "    \n",
    "    #Creating an empty data frames for feature matrices of rare features and common features\n",
    "    rare_feature_df = pd.DataFrame()\n",
    "    common_feature_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range (0, len(feature_list)):\n",
    "        #If the MAF of the feature is less than the cutoff, it will be designated as a rare variant\n",
    "        if feature_df[feature_list[i]].sum()/(2*len(feature_df.index)) < rare_variant_MAF_cutoff and feature_df[feature_list[i]].sum()/(2*len(feature_df.index)) > 0:\n",
    "            rare_feature_list.append(feature_list[i])\n",
    "            rare_feature_MAF_dict[feature_list[i]] = feature_df[feature_list[i]].sum()/(2*len(feature_df.index))\n",
    "            rare_feature_df[feature_list[i]] = feature_df[feature_list[i]]\n",
    "        \n",
    "        elif feature_df[feature_list[i]].sum()/(2*len(feature_df.index)) == 0:\n",
    "            MAF_0_features.append(feature_list[i])\n",
    "\n",
    "        \n",
    "        #Otherwise, it will be considered as a common feature\n",
    "        elif feature_df[feature_list[i]].sum()/(2*len(feature_df.index)) > rare_variant_MAF_cutoff:\n",
    "            common_feature_list.append(feature_list[i])\n",
    "            common_feature_MAF_dict[feature_list[i]] = feature_df[feature_list[i]].sum()/(2*len(feature_df.index))\n",
    "            common_feature_df[feature_list[i]] = feature_df[feature_list[i]]\n",
    "        \n",
    "        #In case the MAF is exactly the cutoff \n",
    "        elif feature_df[feature_list[i]].sum()/(2*len(feature_df.index)) == rare_variant_MAF_cutoff:\n",
    "            common_feature_list.append(feature_list[i])\n",
    "            common_feature_MAF_dict[feature_list[i]] = feature_df[feature_list[i]].sum()/(2*len(feature_df.index))\n",
    "            common_feature_df[feature_list[i]] = feature_df[feature_list[i]]\n",
    "    \n",
    "    #Adding the class label to each data frame\n",
    "    rare_feature_df['Class'] = original_feature_matrix[label_name]\n",
    "    common_feature_df['Class'] = original_feature_matrix[label_name]\n",
    "    return rare_feature_list, rare_feature_MAF_dict, rare_feature_df, common_feature_list, common_feature_MAF_dict, common_feature_df, MAF_0_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Genetic Algorithm with Relief-based Feature Scoring (repeated for a given number of iterations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2a: Relief-based Feature Importance Scoring and Bin Deletion\n",
    "\n",
    "#Use MultiSURF to calculate the feature importance of each candidate bin \n",
    "#If the population size > the set max population size then bins will be probabilistically deleted based on fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate the feature importance of each bin using MultiSURF\n",
    "def MultiSURF_Feature_Importance(bin_feature_matrix, label_name):\n",
    "    \n",
    "    #Converting to float to prevent any errors with the MultiSURF algorithm\n",
    "    float_feature_matrix = bin_feature_matrix.astype(float)\n",
    "    \n",
    "    #Using MultiSURF and storing the feature importances in a dictionary\n",
    "    features, labels = float_feature_matrix.drop(label_name, axis=1).values, float_feature_matrix[label_name].values\n",
    "    fs = MultiSURF()\n",
    "    fs.fit(features, labels)\n",
    "    feature_scores = {}\n",
    "    for feature_name, feature_score in zip(float_feature_matrix.drop(label_name, axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "        feature_scores[feature_name] = feature_score\n",
    "        \n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate MultiSURF feature importance based on a sample of instances\n",
    "def MultiSURF_Feature_Importance_Instance_Sample(bin_feature_matrix, label_name, sample_size):\n",
    "    \n",
    "    #Taking a random sample of the instances based on the sample size paramter to calculate MultiSURF \n",
    "    bin_feature_matrix_sample = bin_feature_matrix.sample(sample_size)\n",
    "    \n",
    "    #Converting to float to prevent any errors with the MultiSURF algorithm\n",
    "    float_feature_matrix = bin_feature_matrix_sample.astype(float)\n",
    "    \n",
    "    #Using MultiSURF and storing the feature importances in a dictionary\n",
    "    features, labels = float_feature_matrix.drop(label_name, axis=1).values, float_feature_matrix[label_name].values\n",
    "    fs = MultiSURF()\n",
    "    fs.fit(features, labels)\n",
    "    feature_scores = {}\n",
    "    for feature_name, feature_score in zip(float_feature_matrix.drop(label_name, axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "        feature_scores[feature_name] = feature_score\n",
    "        \n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate the feature importance of each bin using MultiSURF for rare variants in context with common variables\n",
    "def MultiSURF_Feature_Importance_Rare_Variants(bin_feature_matrix, common_feature_list, common_feature_matrix, label_name):\n",
    "    \n",
    "    #Creating a feature matrix with both binned rare variants and common features when calculating feature importance\n",
    "    common_features_and_bins_matrix = bin_feature_matrix.copy()\n",
    "    \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        common_features_and_bins_matrix[common_feature_list[i]] = common_feature_matrix[common_feature_list[i]]\n",
    "\n",
    "    #Converting to float to prevent any errors with the MultiSURF algorithm\n",
    "    float_feature_matrix = common_features_and_bins_matrix.astype(float)\n",
    "    \n",
    "    #Using MultiSURF and storing the feature importances in a dictionary\n",
    "    features, labels = float_feature_matrix.drop(label_name, axis=1).values, float_feature_matrix[label_name].values\n",
    "    fs = MultiSURF()\n",
    "    fs.fit(features, labels)\n",
    "    feature_scores = {}\n",
    "    for feature_name, feature_score in zip(float_feature_matrix.drop(label_name, axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "        feature_scores[feature_name] = feature_score\n",
    "        \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        del feature_scores[common_feature_list[i]]\n",
    "        \n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate MultiSURF feature importance for rare variants in context with common variables based on a sample of instances\n",
    "def MultiSURF_Feature_Importance_Rare_Variants_Instance_Sample(bin_feature_matrix, common_feature_list, common_feature_matrix,\n",
    "                                                               label_name, sample_size):\n",
    "    \n",
    "    #Creating a feature matrix with both binned rare variants and common features when calculating feature importance\n",
    "    common_features_and_bins_matrix = bin_feature_matrix.copy()\n",
    "    \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        common_features_and_bins_matrix[common_feature_list[i]] = common_feature_matrix[common_feature_list[i]]\n",
    "    \n",
    "    #Taking a random sample of the instances based on the sample size paramter to calculate MultiSURF \n",
    "    common_features_and_bins_matrix_sample = common_features_and_bins_matrix.sample(sample_size)\n",
    "    \n",
    "    #Converting to float to prevent any errors with the MultiSURF algorithm\n",
    "    float_feature_matrix = common_features_and_bins_matrix_sample.astype(float)\n",
    "    \n",
    "    #Using MultiSURF and storing the feature importances in a dictionary\n",
    "    features, labels = float_feature_matrix.drop(label_name, axis=1).values, float_feature_matrix[label_name].values\n",
    "    fs = MultiSURF()\n",
    "    fs.fit(features, labels)\n",
    "    feature_scores = {}\n",
    "    for feature_name, feature_score in zip(float_feature_matrix.drop(label_name, axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "        feature_scores[feature_name] = feature_score\n",
    "    \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        del feature_scores[common_feature_list[i]]\n",
    "        \n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate MultiSURF feature importance only considering the bin and common feature(s)\n",
    "def MultiSURF_Feature_Importance_Bin_and_Common_Features(bin_feature_matrix, amino_acid_bins, common_feature_list, common_feature_matrix, label_name):\n",
    "    #Creating a feature matrix with both binned rare variants and common features when calculating feature importance\n",
    "    common_features_and_bins_matrix = bin_feature_matrix.copy()\n",
    "    \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        common_features_and_bins_matrix[common_feature_list[i]] = common_feature_matrix[common_feature_list[i]]\n",
    "    \n",
    "    bin_scores = {}\n",
    "    for i in amino_acid_bins.keys():\n",
    "        #Only taking the bin and the common features for the feature importance calculation\n",
    "        bin_and_common_features = []\n",
    "        bin_and_common_features.append(i)\n",
    "        bin_and_common_features.extend(common_feature_list)\n",
    "        bin_and_common_features.append(label_name)\n",
    "        bin_and_cf_df = common_features_and_bins_matrix[bin_and_common_features]\n",
    "        float_feature_matrix = bin_and_cf_df.astype(float)\n",
    "        features, labels = float_feature_matrix.drop(label_name, axis=1).values, float_feature_matrix[label_name].values\n",
    "        fs = MultiSURF()\n",
    "        fs.fit(features, labels)\n",
    "        feature_scores = {}\n",
    "        for feature_name, feature_score in zip(float_feature_matrix.drop(label_name, axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "            feature_scores[feature_name] = feature_score\n",
    "        bin_scores[i] = feature_scores[i]\n",
    "    \n",
    "    return bin_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to calculate MultiSURF feature importance only considering the bin and common feature(s)\n",
    "def MultiSURF_Feature_Importance_Bin_and_Common_Features_Instance_Sample(bin_feature_matrix, amino_acid_bins, common_feature_list, common_feature_matrix, label_name, sample_size):\n",
    "    #Creating a feature matrix with both binned rare variants and common features when calculating feature importance\n",
    "    common_features_and_bins_matrix = bin_feature_matrix.copy()\n",
    "    \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        common_features_and_bins_matrix[common_feature_list[i]] = common_feature_matrix[common_feature_list[i]]\n",
    "    \n",
    "    bin_scores = {}\n",
    "    for i in amino_acid_bins.keys():\n",
    "        #Only taking the bin and the common features for the feature importance calculation\n",
    "        bin_and_common_features = []\n",
    "        bin_and_common_features.append(i)\n",
    "        bin_and_common_features.extend(common_feature_list)\n",
    "        bin_and_common_features.append(label_name)\n",
    "        bin_and_cf_df = common_features_and_bins_matrix[bin_and_common_features]\n",
    "        \n",
    "        #Taking a sample to run MultiSURF on\n",
    "        bin_and_cf_df_sample = bin_and_cf_df.sample(sample_size)\n",
    "        float_feature_matrix = bin_and_cf_df_sample.astype(float)\n",
    "        features, labels = float_feature_matrix.drop(label_name, axis=1).values, float_feature_matrix[label_name].values\n",
    "        fs = MultiSURF()\n",
    "        fs.fit(features, labels)\n",
    "        feature_scores = {}\n",
    "        for feature_name, feature_score in zip(float_feature_matrix.drop(label_name, axis=1).columns,\n",
    "                                       fs.feature_importances_):\n",
    "            feature_scores[feature_name] = feature_score\n",
    "        bin_scores[i] = feature_scores[i]\n",
    "    \n",
    "    return bin_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to score bins based on chi squared value\n",
    "def Chi_Square_Feature_Importance(bin_feature_matrix, label_name, amino_acid_bins):\n",
    "    \n",
    "   #Calculating the chisquare and p values of each of the bin features in the bin feature matrix\n",
    "    feature_matrix = bin_feature_matrix.copy()\n",
    "    X = bin_feature_matrix.drop(label_name,axis=1)\n",
    "    y = bin_feature_matrix[label_name]\n",
    "    chi_scores, p_values = chi2(X,y)\n",
    "\n",
    "    #Creating a dictionary with each bin and the chi-square value and p-value\n",
    "    bin_scores = {}\n",
    "    bin_names_list = list(amino_acid_bins.keys())\n",
    "    for i in range (0, len(bin_names_list)):\n",
    "        bin_scores[bin_names_list[i]] = chi_scores[i]\n",
    "        \n",
    "    for i in bin_scores.keys():\n",
    "        if np.isnan(bin_scores[i]) == True:\n",
    "            bin_scores[i] = 0\n",
    "    \n",
    "    return bin_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2b: Genetic Algorithm \n",
    "\n",
    "#Parent bins are probabilistically selected based on fitness (score calculated in Step 2a)\n",
    "#New offspring bins will be created through cross over and mutation and are added to the next generation's population\n",
    "#Based on the value of the elitism parameter, a number of high scoring parent bins will be preserved for the next gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to probabilitically select 2 parent bins based on their feature importance rank\n",
    "#Tournament Selection works in this case by choosing a random sample of the bins and choosing the best two scores\n",
    "def Tournament_Selection_Parent_Bins(bin_scores):\n",
    "    \n",
    "    #Choosing a random sample of 5% of the bin population or if that would be too small, choosing a sample of 50%\n",
    "    if round(0.05*len(bin_scores)) < 2:\n",
    "        samplekeys = random.sample(bin_scores.keys(), round(0.5*len(bin_scores)))\n",
    "    else: \n",
    "        samplekeys = random.sample(bin_scores.keys(), round(0.05*len(bin_scores)))\n",
    "    \n",
    "    sample = {}\n",
    "    for key in samplekeys:\n",
    "        sample[key] = bin_scores[key]\n",
    "    \n",
    "    #Sorting the bins from best score to worst score\n",
    "    sorted_bin_scores = dict(sorted(sample.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_bin_list = list(sorted_bin_scores.keys())\n",
    "    \n",
    "    #Choosing the parent bins and adding them to a list of parent bins\n",
    "    parent_bins = [sorted_bin_list[0], sorted_bin_list[1]]\n",
    "    \n",
    "    return parent_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function for crossover and mutation that creates n offspring based on crossover of selected parents\n",
    "#n is the max number of bins (but not all the offspring will carry on, as the worst will be deleted in Step 2a next time)\n",
    "def Crossover_and_Mutation(max_population_of_bins, elitism_parameter, feature_list, binned_feature_groups, bin_scores, \n",
    "                           crossover_probability, mutation_probability):\n",
    "\n",
    "    #Creating a list for offspring\n",
    "    offspring_list = []\n",
    "    \n",
    "    #Creating a number of offspring equal to the the number needed to replace the nonelites\n",
    "    #Each pair of parents will produce two offspring\n",
    "    for i in range (0, int((max_population_of_bins - (elitism_parameter*max_population_of_bins))/2)):\n",
    "        #Choosing the two parents and gettings the list of features in each parent bin\n",
    "        parent_bins = Tournament_Selection_Parent_Bins(bin_scores)\n",
    "        parent1_features = binned_feature_groups[parent_bins[0]].copy()\n",
    "        parent2_features = binned_feature_groups[parent_bins[1]].copy()\n",
    "\n",
    "        #Creating two lists for the offspring bins\n",
    "        offspring1 = []\n",
    "        offspring2 = []\n",
    "        \n",
    "        #CROSSOVER\n",
    "        #Each feature in the parent bin will crossover based on the given probability (uniform crossover)\n",
    "        for j in range (0, len(parent1_features)):\n",
    "            if crossover_probability > random.random():\n",
    "                offspring2.append(parent1_features[j])\n",
    "            else:\n",
    "                offspring1.append(parent1_features[j])\n",
    "        \n",
    "        for k in range (0, len(parent2_features)):\n",
    "            if crossover_probability > random.random():\n",
    "                offspring1.append(parent2_features[k])\n",
    "            else:\n",
    "                offspring2.append(parent2_features[k])\n",
    "        \n",
    "        #Ensuring that each of the offspring is no more than twice the size of the other offspring\n",
    "        while len(offspring1) > len(offspring2):\n",
    "            switch = random.choice(offspring1)\n",
    "            offspring1.remove(switch)\n",
    "            offspring2.append(switch)\n",
    "            \n",
    "        while len(offspring2) > len(offspring1):\n",
    "            switch = random.choice(offspring2)\n",
    "            offspring2.remove(switch)\n",
    "            offspring1.append(switch)\n",
    "        \n",
    "        \n",
    "        #MUTATION\n",
    "        #Mutation only occurs with a certain probability on each feature in the original feature space\n",
    "        \n",
    "        #Applying the mutation operation to the first offspring\n",
    "        #Creating a probability for adding a feature that accounts for the ratio between the feature list and the size of the bin\n",
    "        if len(offspring1) > 0 and len(offspring1) != len(feature_list):            \n",
    "            mutation_addition_prob = (mutation_probability)*(len(offspring1))/((len(feature_list)-len(offspring1)))\n",
    "        elif len(offspring1) == 0 and len(offspring1) != len(feature_list):\n",
    "            mutation_addition_prob = mutation_probability\n",
    "        elif len(offspring1) == len(feature_list):\n",
    "            mutation_addition_prob = 0\n",
    "        \n",
    "        deleted_list = []\n",
    "        #Deletion form of mutation\n",
    "        for l in range (0, len(offspring1)):\n",
    "            #Mutation (deletion) occurs on this feature with probability equal to the mutation parameter\n",
    "            if mutation_probability > random.random():\n",
    "                deleted_list.append(offspring1[l])\n",
    "                \n",
    "        \n",
    "        for l in range (0, len(deleted_list)):\n",
    "            offspring1.remove(deleted_list[l])\n",
    "            \n",
    "        #Creating a list of features outside the offspring\n",
    "        features_not_in_offspring = [item for item in feature_list if item not in offspring1]\n",
    "        \n",
    "        #Addition form of mutation\n",
    "        for l in range (0, len(features_not_in_offspring)):\n",
    "            #Mutation (addiiton) occurs on this feature with probability proportional to the mutation parameter\n",
    "            #The probability accounts for the ratio between the feature list and the size of the bin\n",
    "            if mutation_addition_prob > random.random():\n",
    "                    offspring1.append(features_not_in_offspring[l])\n",
    "        \n",
    "        #Applying the mutation operation to the second offspring\n",
    "        #Creating a probability for adding a feature that accounts for the ratio between the feature list and the size of the bin\n",
    "        if len(offspring2) > 0 and len(offspring2) != len(feature_list):            \n",
    "            mutation_addition_prob = (mutation_probability)*(len(offspring2))/((len(feature_list)-len(offspring2)))\n",
    "        elif len(offspring2) == 0 and len(offspring2) != len(feature_list):\n",
    "            mutation_addition_prob = mutation_probability\n",
    "        elif len(offspring2) == len(feature_list):\n",
    "            mutation_addition_prob = 0\n",
    "        \n",
    "        deleted_list = []\n",
    "        #Deletion form of mutation\n",
    "        for l in range (0, len(offspring2)):\n",
    "            #Mutation (deletion) occurs on this feature with probability equal to the mutation parameter\n",
    "            if mutation_probability > random.random():\n",
    "                deleted_list.append(offspring2[l])\n",
    "        \n",
    "        for l in range (0, len(deleted_list)):\n",
    "            offspring2.remove(deleted_list[l])\n",
    "        \n",
    "        #Creating a list of features outside the offspring\n",
    "        features_not_in_offspring = [item for item in feature_list if item not in offspring2]\n",
    "        \n",
    "        #Addition form of mutation\n",
    "        for l in range (0, len(features_not_in_offspring)):\n",
    "            #Mutation (addiiton) occurs on this feature with probability proportional to the mutation parameter\n",
    "            #The probability accounts for the ratio between the feature list and the size of the bin\n",
    "            if mutation_addition_prob > random.random():\n",
    "                    offspring2.append(features_not_in_offspring[l])\n",
    "            \n",
    "        #CLEANUP\n",
    "        #Deleting any repeats of an amino acid in a bin\n",
    "        #Removing duplicates of features in the same bin that may arise due to crossover\n",
    "        unique = []\n",
    "        for a in range (0, len(offspring1)):\n",
    "            if offspring1[a] not in unique:\n",
    "                unique.append(offspring1[a])\n",
    "        \n",
    "        #Adding random features from outside the bin to replace the deleted features in the bin\n",
    "        replace_number = len(offspring1) - len(unique)\n",
    "        features_not_in_offspring = []\n",
    "        features_not_in_offspring = [item for item in feature_list if item not in offspring1]\n",
    "        offspring1 = unique\n",
    "        if len(features_not_in_offspring) > replace_number:\n",
    "            replacements = random.sample(features_not_in_offspring, replace_number)\n",
    "        else:\n",
    "            replacements = features_not_in_offspring.copy()\n",
    "        offspring1.extend(replacements)\n",
    "        \n",
    "        unique = []\n",
    "        for a in range (0, len(offspring2)):\n",
    "            if offspring2[a] not in unique:\n",
    "                unique.append(offspring2[a])\n",
    "        \n",
    "        #Adding random features from outside the bin to replace the deleted features in the bin\n",
    "        replace_number = len(offspring2) - len(unique)\n",
    "        features_not_in_offspring = []\n",
    "        features_not_in_offspring = [item for item in feature_list if item not in offspring2]\n",
    "        offspring2 = unique\n",
    "        if len(features_not_in_offspring) > replace_number:\n",
    "            replacements = random.sample(features_not_in_offspring, replace_number)\n",
    "        else:\n",
    "            replacements = features_not_in_offspring.copy()\n",
    "        offspring2.extend(replacements)\n",
    "        \n",
    "        #Adding the new offspring to the list of feature bins\n",
    "        offspring_list.append(offspring1)\n",
    "        offspring_list.append(offspring2)\n",
    "        \n",
    "   \n",
    "    return offspring_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Next_Generation(binned_feature_groups, bin_scores, max_population_of_bins, elitism_parameter, offspring_list):\n",
    "    \n",
    "    #Sorting the bins from best score to worst score\n",
    "    sorted_bin_scores = dict(sorted(bin_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_bin_list = list(sorted_bin_scores.keys())\n",
    "        \n",
    "    #Determining the number of elite bins\n",
    "    number_of_elite_bins = round(max_population_of_bins*elitism_parameter)\n",
    "    elites = []\n",
    "    #Adding the elites to a list of elite feature bins\n",
    "    for a in range (0, number_of_elite_bins):\n",
    "        elites.append(binned_feature_groups[sorted_bin_list[a]])\n",
    "    \n",
    "    #Creating a list of feature bins (without labels because those will be changed as things get deleted and added)\n",
    "    feature_bin_list = elites.copy()\n",
    "    \n",
    "    #Adding the offspring to the feature bin list\n",
    "    feature_bin_list.extend(offspring_list)\n",
    "    return feature_bin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to recreate the feature matrix (add up values of amino a cids from original dataset)\n",
    "def Regroup_Feature_Matrix(feature_list, feature_matrix, label_name, feature_bin_list):\n",
    "    \n",
    "    \n",
    "    #First deleting any bins that are empty\n",
    "    #Creating a list of bins to delete\n",
    "    bins_to_delete = []\n",
    "    for i in feature_bin_list:\n",
    "        if not i:\n",
    "            bins_to_delete.append(i)\n",
    "    for i in bins_to_delete:\n",
    "        feature_bin_list.remove(i)\n",
    "    \n",
    "    #The length of the bin will be equal to the average length of nonempty bins in the population\n",
    "    bin_lengths = []\n",
    "    for i in feature_bin_list:\n",
    "        if len(i) > 0:\n",
    "            bin_lengths.append(len(i))      \n",
    "    replacement_length = round(statistics.mean(bin_lengths))\n",
    "    \n",
    "    #Replacing each deleted bin with a bin with random features\n",
    "    for i in range (0, len(bins_to_delete)):\n",
    "        replacement = random.sample(feature_list, replacement_length)\n",
    "        feature_bin_list.append(replacement)\n",
    "\n",
    "    #Checking each pair of bins, if the bins are duplicates then one of the copies will be deleted\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for x in feature_bin_list:\n",
    "        srtd = tuple(sorted(x))\n",
    "        if srtd not in seen:\n",
    "            unique.append(x)\n",
    "            seen.add(srtd)\n",
    "    \n",
    "    #Replacing each deleted bin with a bin with random features\n",
    "    replacement_number = len(feature_bin_list) - len(unique)\n",
    "    feature_bin_list = unique.copy()\n",
    "    \n",
    "    for i in feature_bin_list:\n",
    "        if len(i) > 0:\n",
    "            bin_lengths.append(len(i))      \n",
    "    replacement_length = round(statistics.mean(bin_lengths))\n",
    "    \n",
    "    for i in range(0, replacement_number):\n",
    "        replacement = random.sample(feature_list, replacement_length)\n",
    "        feature_bin_list.append(replacement)\n",
    "            \n",
    "    #Creating an empty data frame for the feature matrix with bins\n",
    "    bins_df = pd.DataFrame()\n",
    "    \n",
    "    #Creating a list of 0s, where the number of 0s is the number of instances in the original feature matrix\n",
    "    zero_list = []\n",
    "    for a in range (0, len(feature_matrix.index)):\n",
    "        zero_list.append(0)\n",
    "        \n",
    "    #Creating a dummy data frame\n",
    "    dummy_df = pd.DataFrame()\n",
    "    dummy_df['Zeros'] = zero_list\n",
    "    #The list and dummy data frame will be used for adding later\n",
    "    \n",
    "   #For each feature group/bin, the values of the amino acid in the bin will be summed to create a value for the bin\n",
    "   #This will be used to create a a feature matrix for the bins and a dictionary of binned feature groups\n",
    "\n",
    "    count = 0\n",
    "    binned_feature_groups = {}\n",
    "    \n",
    "    for i in range (0, len(feature_bin_list)):\n",
    "        sum_column = dummy_df['Zeros']\n",
    "        for j in range (0, len(feature_bin_list[i])):\n",
    "            sum_column = sum_column + feature_matrix[feature_bin_list[i][j]]\n",
    "        count = count + 1\n",
    "        bins_df[\"Bin \" + str(count)] = sum_column\n",
    "        binned_feature_groups[\"Bin \" + str(count)] = feature_bin_list[i]\n",
    "    \n",
    "    #Adding the class label to the data frame\n",
    "    bins_df['Class'] = feature_matrix[label_name]\n",
    "    return bins_df, binned_feature_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Defining a function for the RAFE algorithm (Relief-based Association Feature-bin Evolver)\n",
    "#Same as RARE but it bins all features (both common and rare) not just rare features\n",
    "def RAFE (given_starting_point, amino_acid_start_point, amino_acid_bins_start_point, iterations, original_feature_matrix, \n",
    "          label_name, set_number_of_bins, min_features_per_group, max_number_of_groups_with_feature, \n",
    "          scoring_method, score_based_on_sample, instance_sample_size, \n",
    "          crossover_probability, mutation_probability, elitism_parameter):\n",
    "\n",
    "    #Step 0: Deleting Empty Features (MAF = 0)\n",
    "    feature_matrix_no_empty_variables, MAF_0_features, nonempty_feature_list = Remove_Empty_Variables(original_feature_matrix, \n",
    "                                                                                                      'Class')\n",
    "\n",
    "    #Step 1: Initialize Population of Candidate Bins \n",
    "    #Initialize Feature Groups\n",
    "    \n",
    "    #If there is a starting point, use that for the amino acid list and the amino acid bins list\n",
    "    if given_starting_point == True:\n",
    "        amino_acid_bins = amino_acid_bins_start_point.copy()\n",
    "        amino_acids = amino_acid_start_point.copy()\n",
    "        \n",
    "        features_to_remove = [item for item in amino_acids if item not in nonempty_feature_list]\n",
    "        for feature in features_to_remove:\n",
    "            amino_acids.remove(feature)\n",
    "                \n",
    "        bin_names = amino_acid_bins.keys()\n",
    "        for bin_name in bin_names:\n",
    "            for feature in features_to_remove:\n",
    "                if feature in amino_acid_bins[bin_name]:\n",
    "                    amino_acid_bins[bin_name].remove(feature)\n",
    "                    \n",
    "    #Otherwise randomly initialize the bins\n",
    "    elif given_starting_point == False:\n",
    "        amino_acids, amino_acid_bins = Random_Feature_Grouping(feature_matrix_no_empty_variables, label_name, \n",
    "                                                               set_number_of_bins, min_features_per_group, \n",
    "                                                               max_number_of_groups_with_feature)\n",
    "    \n",
    "    #Create Initial Binned Feature Matrix\n",
    "    bin_feature_matrix = Grouped_Feature_Matrix(feature_matrix_no_empty_variables, label_name, amino_acid_bins)\n",
    "    \n",
    "    #Step 2: Genetic Algorithm with Feature Scoring (repeated for a given number of iterations)\n",
    "    for i in range (0, iterations):\n",
    "        \n",
    "        #Step 2a: Feature Importance Scoring and Bin Deletion\n",
    "        \n",
    "        #Feature scoring can be done with Relief or a univariate chi squared test\n",
    "        if scoring_method == 'Relief':\n",
    "            #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "            if score_based_on_sample == False:\n",
    "                amino_acid_bin_scores = MultiSURF_Feature_Importance(bin_feature_matrix, label_name)\n",
    "            elif score_based_on_sample == True:\n",
    "                amino_acid_bin_scores = MultiSURF_Feature_Importance_Instance_Sample(bin_feature_matrix, label_name,\n",
    "                                                                                instance_sample_size)\n",
    "        elif scoring_method == 'Univariate':\n",
    "            amino_acid_bin_scores = Chi_Square_Feature_Importance(bin_feature_matrix, 'Class', amino_acid_bins)\n",
    "            \n",
    "        #Step 2b: Genetic Algorithm \n",
    "        #Creating the offspring bins through crossover and mutation\n",
    "        offspring_bins = Crossover_and_Mutation(set_number_of_bins, elitism_parameter, amino_acids, amino_acid_bins, amino_acid_bin_scores,\n",
    "                                                crossover_probability, mutation_probability)\n",
    "        \n",
    "        #Creating the new generation by preserving some elites and adding the offspring\n",
    "        feature_bin_list = Create_Next_Generation(amino_acid_bins, amino_acid_bin_scores, set_number_of_bins, \n",
    "                                                  elitism_parameter, offspring_bins)\n",
    "        \n",
    "        bin_feature_matrix, amino_acid_bins = Regroup_Feature_Matrix(amino_acids, original_feature_matrix, label_name, feature_bin_list)\n",
    "    \n",
    "    #Creating the final amino acid bin scores\n",
    "    #Feature scoring can be done with Relief or a univariate chi squared test\n",
    "    if scoring_method == 'Relief':\n",
    "        #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "        if score_based_on_sample == False:\n",
    "            amino_acid_bin_scores = MultiSURF_Feature_Importance(bin_feature_matrix, label_name)\n",
    "        elif score_based_on_sample == True:\n",
    "            amino_acid_bin_scores = MultiSURF_Feature_Importance_Instance_Sample(bin_feature_matrix, label_name,\n",
    "                                                                                instance_sample_size)\n",
    "    elif scoring_method == 'Univariate':\n",
    "        amino_acid_bin_scores = Chi_Square_Feature_Importance(bin_feature_matrix, 'Class', amino_acid_bins)\n",
    "    \n",
    "    return bin_feature_matrix, amino_acid_bins, amino_acid_bin_scores, MAF_0_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\satvi\\\\Downloads\\\\SRTR_data.csv' does not exist: b'C:\\\\Users\\\\satvi\\\\Downloads\\\\SRTR_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c46a4df89a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moriginal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:\\\\Users\\\\satvi\\\\Downloads\\\\SRTR_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\satvi\\\\Downloads\\\\SRTR_data.csv' does not exist: b'C:\\\\Users\\\\satvi\\\\Downloads\\\\SRTR_data.csv'"
     ]
    }
   ],
   "source": [
    "original_df = pd.read_csv('C:\\\\Users\\\\satvi\\\\Downloads\\\\SRTR_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to present the top bins \n",
    "#This should be used after RAFE\n",
    "def Top_Bins_Summary(original_feature_matrix, label_name, bin_feature_matrix, bins, bin_scores, number_of_top_bins):\n",
    "    \n",
    "    #Ordering the bin scores from best to worst\n",
    "    sorted_bin_scores = dict(sorted(bin_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_bin_list = list(sorted_bin_scores.keys())\n",
    "    sorted_bin_feature_importance_values = list(sorted_bin_scores.values())\n",
    "\n",
    "    #Calculating the chi square and p values of each of the features in the original feature matrix\n",
    "    df = original_feature_matrix\n",
    "    X = df.drop('Class',axis=1)\n",
    "    y = df['Class']\n",
    "    chi_scores, p_values = chi2(X,y)\n",
    "    \n",
    "    #Removing the label column to create a list of features\n",
    "    feature_df = original_feature_matrix.drop(columns = [label_name])\n",
    "    \n",
    "    #Creating a list of features \n",
    "    feature_list = []\n",
    "    for column in feature_df:\n",
    "        feature_list.append(str(column))\n",
    "        \n",
    "    #Creating a dictionary with each feature and the chi-square value and p-value\n",
    "    Univariate_Feature_Stats = {}\n",
    "    for i in range (0, len(feature_list)):\n",
    "        list_of_stats = []\n",
    "        list_of_stats.append(chi_scores[i])\n",
    "        list_of_stats.append(p_values[i])\n",
    "        Univariate_Feature_Stats[feature_list[i]] = list_of_stats\n",
    "        #There will be features with nan for their chi-square value and p-value because the whole column is zeroes\n",
    "    \n",
    "    #Calculating the chisquare and p values of each of the features in the bin feature matrix\n",
    "    X = bin_feature_matrix.drop('Class',axis=1)\n",
    "    y = bin_feature_matrix['Class']\n",
    "    chi_scores, p_values = chi2(X,y)\n",
    "\n",
    "    #Creating a dictionary with each bin and the chi-square value and p-value\n",
    "    Bin_Stats = {}\n",
    "    bin_names_list = list(amino_acid_bins.keys())\n",
    "    for i in range (0, len(bin_names_list)):\n",
    "        list_of_stats = []\n",
    "        list_of_stats.append(chi_scores[i])\n",
    "        list_of_stats.append(p_values[i])\n",
    "        Bin_Stats[bin_names_list[i]] = list_of_stats\n",
    "\n",
    "    for i in range (0, number_of_top_bins):\n",
    "        #Printing the bin Name\n",
    "        print (\"Bin Rank \" + str(i+1) + \": \" + sorted_bin_list[i])\n",
    "        #Printing the bin's MultiSURF score, chi-square value, and p-value\n",
    "        print (\"MultiSURF Score: \" + str(sorted_bin_feature_importance_values[i]) + \"; chi-square value: \" + str(Bin_Stats[sorted_bin_list[i]][0]) + \"; p-value: \" + str(Bin_Stats[sorted_bin_list[i]][1]))\n",
    "        #Printing each of the features in the bin and also printing the univariate stats of that feature\n",
    "        for j in range (0, len(bins[sorted_bin_list[i]])):\n",
    "            print (\"Feature Name: \" + bins[sorted_bin_list[i]][j] + \"; chi-square value: \" + str(Univariate_Feature_Stats[bins[sorted_bin_list[i]][j]][0]) + \"; p-value: \" + str(Univariate_Feature_Stats[bins[sorted_bin_list[i]][j]][1]))\n",
    "        print ('---------------------------')                                              \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function for the RARE algorithm (Relief-based Association Rare-variant-bin Evolver)\n",
    "def RARE (given_starting_point, amino_acid_start_point, amino_acid_bins_start_point, iterations, original_feature_matrix, \n",
    "          label_name, rare_variant_MAF_cutoff, set_number_of_bins, \n",
    "          min_features_per_group, max_number_of_groups_with_feature, \n",
    "          scoring_method, score_based_on_sample, score_with_common_variables, \n",
    "          instance_sample_size, crossover_probability, mutation_probability, elitism_parameter):\n",
    "    \n",
    "    #Step 0: Separate Rare Variants and Common Features\n",
    "    rare_feature_list, rare_feature_MAF_dict, rare_feature_df, common_feature_list, common_feature_MAF_dict, common_feature_df, MAF_0_features = Rare_and_Common_Variable_Separation (original_feature_matrix, label_name, rare_variant_MAF_cutoff)\n",
    "\n",
    "    #Step 1: Initialize Population of Candidate Bins \n",
    "    #Initialize Feature Groups\n",
    "    if given_starting_point == True:\n",
    "        amino_acid_bins = amino_acid_bins_start_point.copy()\n",
    "        amino_acids = amino_acid_start_point.copy()\n",
    "        \n",
    "        features_to_remove = [item for item in amino_acids if item not in rare_feature_list]\n",
    "        for feature in features_to_remove:\n",
    "            amino_acids.remove(feature)\n",
    "                \n",
    "        bin_names = amino_acid_bins.keys()\n",
    "        for bin_name in bin_names:\n",
    "            for feature in features_to_remove:\n",
    "                if feature in amino_acid_bins[bin_name]:\n",
    "                    amino_acid_bins[bin_name].remove(feature)\n",
    "    \n",
    "    #Otherwise randomly initialize the bins\n",
    "    elif given_starting_point == False:\n",
    "        amino_acids, amino_acid_bins = Random_Feature_Grouping(rare_feature_df, label_name, \n",
    "                                                               set_number_of_bins, min_features_per_group, \n",
    "                                                               max_number_of_groups_with_feature)\n",
    "    #Create Initial Binned Feature Matrix\n",
    "    bin_feature_matrix = Grouped_Feature_Matrix(rare_feature_df, label_name, amino_acid_bins)\n",
    "    \n",
    "    #Step 2: Genetic Algorithm with Feature Scoring (repeated for a given number of iterations)\n",
    "    for i in range (0, iterations):\n",
    "        \n",
    "        #Step 2a: Feature Importance Scoring and Bin Deletion\n",
    "        \n",
    "        #Feature importance can be scored with Relief or a univariate metric (chi squared value)\n",
    "        if scoring_method == 'Relief':\n",
    "            #Feature importance is calculating either with common variables\n",
    "            if score_with_common_variables == True:\n",
    "                #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "                if score_based_on_sample == False:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance_Rare_Variants(bin_feature_matrix, common_feature_list,\n",
    "                                                                                      common_feature_df, label_name)\n",
    "                elif score_based_on_sample == True:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance_Rare_Variants_Instance_Sample(bin_feature_matrix, \n",
    "                                                                                                  common_feature_list,\n",
    "                                                                                                  common_feature_df,\n",
    "                                                                                                  label_name, \n",
    "                                                                                                  instance_sample_size)\n",
    "        \n",
    "            #Or feauture importance is calculated only based on rare variant bins\n",
    "            elif score_with_common_variables == False:\n",
    "                #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "                if score_based_on_sample == False:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance(bin_feature_matrix, label_name)\n",
    "                elif score_based_on_sample == True:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance_Instance_Sample(bin_feature_matrix, label_name, \n",
    "                                                                                         instance_sample_size)\n",
    "        elif scoring_method == 'Relief only on bin and common features':\n",
    "            #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "            if score_based_on_sample == True:\n",
    "                amino_acid_bin_scores = MultiSURF_Feature_Importance_Bin_and_Common_Features_Instance_Sample(bin_feature_matrix, amino_acid_bins, common_feature_list, common_feature_df, label_name, instance_sample_size)\n",
    "            elif score_based_on_sample == False:\n",
    "                amino_acid_bin_scores = MultiSURF_Feature_Importance_Bin_and_Common_Features(bin_feature_matrix, amino_acid_bins, common_feature_list, common_feature_df, label_name)\n",
    "        \n",
    "        elif scoring_method == 'Univariate':\n",
    "            amino_acid_bin_scores = Chi_Square_Feature_Importance(bin_feature_matrix, 'Class', amino_acid_bins)\n",
    "        \n",
    "        #Step 2b: Genetic Algorithm \n",
    "        #Creating the offspring bins through crossover and mutation\n",
    "        offspring_bins = Crossover_and_Mutation(set_number_of_bins, elitism_parameter, amino_acids, amino_acid_bins, amino_acid_bin_scores,\n",
    "                                                crossover_probability, mutation_probability)\n",
    "        \n",
    "        #Creating the new generation by preserving some elites and adding the offspring\n",
    "        feature_bin_list = Create_Next_Generation(amino_acid_bins, amino_acid_bin_scores, set_number_of_bins, \n",
    "                                                  elitism_parameter, offspring_bins)\n",
    "        \n",
    "        #Updating the binned feature matrix\n",
    "        bin_feature_matrix, amino_acid_bins = Regroup_Feature_Matrix(amino_acids, rare_feature_df, label_name, feature_bin_list)\n",
    "    \n",
    "    \n",
    "    #Creating the final amino acid bin scores\n",
    "    if scoring_method == 'Relief':\n",
    "            #Feature importance is calculating either with common variables\n",
    "            if score_with_common_variables == True:\n",
    "                #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "                if score_based_on_sample == False:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance_Rare_Variants(bin_feature_matrix, common_feature_list,\n",
    "                                                                                      common_feature_df, label_name)\n",
    "                elif score_based_on_sample == True:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance_Rare_Variants_Instance_Sample(bin_feature_matrix, \n",
    "                                                                                                  common_feature_list,\n",
    "                                                                                                  common_feature_df,\n",
    "                                                                                                  label_name, \n",
    "                                                                                                  instance_sample_size)\n",
    "        \n",
    "            #Or feauture importance is calculated only based on rare variant bins\n",
    "            elif score_with_common_variables == False:\n",
    "                #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "                if score_based_on_sample == False:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance(bin_feature_matrix, label_name)\n",
    "                elif score_based_on_sample == True:\n",
    "                    amino_acid_bin_scores = MultiSURF_Feature_Importance_Instance_Sample(bin_feature_matrix, label_name, \n",
    "                                                                                         instance_sample_size)\n",
    "    elif scoring_method == 'Univariate':\n",
    "            amino_acid_bin_scores = Chi_Square_Feature_Importance(bin_feature_matrix, 'Class', amino_acid_bins)\n",
    "    \n",
    "    elif scoring_method == 'Relief only on bin and common features':\n",
    "            #Calculating feature importance with MultiSURF on whole dataset or MultiSURF on a sample\n",
    "            if score_based_on_sample == True:\n",
    "                amino_acid_bin_scores = MultiSURF_Feature_Importance_Bin_and_Common_Features_Instance_Sample(bin_feature_matrix, amino_acid_bins, common_feature_list, common_feature_df, label_name, instance_sample_size)\n",
    "            elif score_based_on_sample == False:\n",
    "                amino_acid_bin_scores = MultiSURF_Feature_Importance_Bin_and_Common_Features(bin_feature_matrix, amino_acid_bins, common_feature_list, common_feature_df, label_name)\n",
    "        \n",
    "    #Creating a final feature matrix with both rare variant bins and common features\n",
    "    common_features_and_bins_matrix = bin_feature_matrix.copy()    \n",
    "    for i in range (0, len(common_feature_list)):\n",
    "        common_features_and_bins_matrix[common_feature_list[i]] = common_feature_df[common_feature_list[i]]\n",
    "    \n",
    "    bin_feature_matrix['Class'] = original_feature_matrix[label_name]\n",
    "    common_features_and_bins_matrix['Class'] = original_feature_matrix[label_name]\n",
    "\n",
    "    return bin_feature_matrix, common_features_and_bins_matrix, amino_acid_bins, amino_acid_bin_scores, rare_feature_MAF_dict, common_feature_MAF_dict, rare_feature_df, common_feature_df, MAF_0_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to present the top bins \n",
    "#This should be used after RARE\n",
    "def Top_Rare_Variant_Bins_Summary(rare_feature_matrix, label_name, bins, bin_scores, \n",
    "                                  rare_feature_MAF_dict, number_of_top_bins):\n",
    "    \n",
    "    #Ordering the bin scores from best to worst\n",
    "    sorted_bin_scores = dict(sorted(bin_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_bin_list = list(sorted_bin_scores.keys())\n",
    "    sorted_bin_feature_importance_values = list(sorted_bin_scores.values())\n",
    "\n",
    "    #Calculating the chi square and p values of each of the features in the rare feature matrix\n",
    "    df = rare_feature_matrix\n",
    "    X = df.drop('Class',axis=1)\n",
    "    y = df['Class']\n",
    "    chi_scores, p_values = chi2(X,y)\n",
    "    \n",
    "    #Removing the label column to create a list of features\n",
    "    feature_df = rare_feature_df.drop(columns = [label_name])\n",
    "    \n",
    "    #Creating a list of features\n",
    "    feature_list = []\n",
    "    for column in feature_df:\n",
    "        feature_list.append(str(column))\n",
    "        \n",
    "    #Creating a dictionary with each feature and the chi-square value and p-value\n",
    "    Univariate_Feature_Stats = {}\n",
    "    for i in range (0, len(feature_list)):\n",
    "        list_of_stats = []\n",
    "        list_of_stats.append(chi_scores[i])\n",
    "        list_of_stats.append(p_values[i])\n",
    "        Univariate_Feature_Stats[feature_list[i]] = list_of_stats\n",
    "        #There will be features with nan for their chi-square value and p-value because the whole column is zeroes\n",
    "    \n",
    "    #Calculating the chisquare and p values of each of the features in the bin feature matrix\n",
    "    X = bin_feature_matrix.drop('Class',axis=1)\n",
    "    y = bin_feature_matrix['Class']\n",
    "    chi_scores, p_values = chi2(X,y)\n",
    "\n",
    "    #Creating a dictionary with each bin and the chi-square value and p-value\n",
    "    Bin_Stats = {}\n",
    "    bin_names_list = list(amino_acid_bins.keys())\n",
    "    for i in range (0, len(bin_names_list)):\n",
    "        list_of_stats = []\n",
    "        list_of_stats.append(chi_scores[i])\n",
    "        list_of_stats.append(p_values[i])\n",
    "        Bin_Stats[bin_names_list[i]] = list_of_stats\n",
    "    \n",
    "    for i in range (0, number_of_top_bins):\n",
    "        #Printing the bin Name\n",
    "        print (\"Bin Rank \" + str(i+1) + \": \" + sorted_bin_list[i])\n",
    "        #Printing the bin's MultiSURF score, chi-square value, and p-value\n",
    "        print (\"MultiSURF Score: \" + str(sorted_bin_feature_importance_values[i]) + \"; chi-square value: \" + str(Bin_Stats[sorted_bin_list[i]][0]) + \"; p-value: \" + str(Bin_Stats[sorted_bin_list[i]][1]))\n",
    "        #Printing each of the features in the bin and also printing the univariate stats of that feature\n",
    "        for j in range (0, len(bins[sorted_bin_list[i]])):\n",
    "            print (\"Feature Name: \" + str(bins[sorted_bin_list[i]][j]) + \"; minor allele frequency: \" + str(rare_feature_MAF_dict[bins[sorted_bin_list[i]][j]]) + \"; chi-square value: \" + str(Univariate_Feature_Stats[bins[sorted_bin_list[i]][j]][0]) + \"; p-value: \" + str(Univariate_Feature_Stats[bins[sorted_bin_list[i]][j]][1]))\n",
    "        print ('---------------------------')                                              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
